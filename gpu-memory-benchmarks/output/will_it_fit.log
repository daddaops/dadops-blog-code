
============================================================
INFERENCE SCENARIOS
============================================================

==================================================
  7B model | FP16 | Inference
  Batch=1, Seq=512
==================================================
  Parameters            14.0 GB  ############################
  KV Cache               0.3 GB  
  ────────────────────────────────────────
  TOTAL                 14.3 GB
  GPU VRAM:             24.0 GB
  Status:            ✓ FITS

==================================================
  7B model | FP16 | Inference
  Batch=8, Seq=2048
==================================================
  Parameters            14.0 GB  ############################
  KV Cache               8.6 GB  #################
  ────────────────────────────────────────
  TOTAL                 22.6 GB
  GPU VRAM:             24.0 GB
  Status:            ✓ FITS

==================================================
  7B model | INT4 | Inference
  Batch=1, Seq=2048
==================================================
  Parameters             3.5 GB  #######
  KV Cache               0.3 GB  
  ────────────────────────────────────────
  TOTAL                  3.8 GB
  GPU VRAM:             24.0 GB
  Status:            ✓ FITS

============================================================
TRAINING SCENARIOS
============================================================

==================================================
  7B model | FP16 | Training
  Batch=4, Seq=512
==================================================
  Parameters            14.0 GB  ############################
  Optimizer States      84.0 GB  ########################################################################################################################################################################
  Gradients             14.0 GB  ############################
  Activations            6.4 GB  ############
  ────────────────────────────────────────
  TOTAL                118.4 GB
  GPU VRAM:             80.0 GB
  Status:            ✗ OOM

==================================================
  7B model | FP16 | Training
  Batch=1, Seq=2048
==================================================
  Parameters            14.0 GB  ############################
  Optimizer States      84.0 GB  ########################################################################################################################################################################
  Gradients             14.0 GB  ############################
  Activations            6.4 GB  ############
  ────────────────────────────────────────
  TOTAL                118.4 GB
  GPU VRAM:             80.0 GB
  Status:            ✗ OOM

==================================================
  1.3B model | FP16 | Training
  Batch=8, Seq=512
==================================================
  Parameters             2.6 GB  #####
  Optimizer States      15.6 GB  ###############################
  Gradients              2.6 GB  #####
  Activations           12.9 GB  #########################
  ────────────────────────────────────────
  TOTAL                 33.7 GB
  GPU VRAM:             24.0 GB
  Status:            ✗ OOM
